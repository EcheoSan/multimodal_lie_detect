研究摘要（Research Abstract）
標題：
多模態情緒一致性分析於語句真偽辨識之初探

摘要：
本研究旨在探討使用多模態情緒辨識技術來推論語句的真偽可行性。研究假設基於一個核心觀點：當個體說謊時，其語意、聲音與臉部表情間的情緒表現將出現顯著不一致性。我們建構了一套自動化分析流程，從影片中抽取語音、轉錄文字與影像畫面，並分別透過語音情緒分類模型（superb/wav2vec2-base-superb-er）、文本情緒分類模型（j-hartmann/emotion-english-distilroberta-base）與 DeepFace 臉部情緒分析模型，計算各模態情緒分布向量，再以餘弦相似度衡量其間一致性。

本研究以一組人工標記為「真話」與「假話」的影片樣本作為實驗資料，透過批次處理產出每段影片的多模態一致性指標（音訊與文本、音訊與臉部、文本與臉部）。預期結果為：「假話組」的模態間相似度顯著低於「真話組」，並以此作為判別依據。

初步結果：
初步結果顯示，部分假話影片在多模態一致性上呈現偏低的相似度，然而目前仍無法形成清晰的門檻規則。情緒分類模型之輸出分布、樣本品質、語言模型對非英文語句的敏感度與情緒標記不一致等因素，皆可能影響推論準確度。

結論與未來方向：
多模態一致性在理論上具備辨識潛力，但受限於現有模型精度與特定情境變數，尚需進一步調整模型參數、使用更嚴謹的語料、建立基準比對樣本與引入統計檢定方法，以提升辨識可信度與泛化能力。